# -*- coding: utf-8 -*-
"""NETFLIX MOVIES AND TV SHOWS CLUSTERING_Arun_Chaudhary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Todh_k0q6VP3jhnZNXVz5WUSCkkfLiBX

# **Project Name**    - **NETFLIX MOVIES AND TV SHOWS CLUSTERING**

# **Project Summary -**

This is an Unsupervised Machine learning project. In this project i will have to build a model that can be capable of clustering different-different types of data. The dataset is about netflix shows which has 7787 rows and 12 columns like show_id which represents ID of the show, type represents type of the show, title represents show title, cast represents name of the casting stars, country represents the country of the show, date added represents the date when the show is added to netflix, release-year represents the year the show was released,rating represents the rating of the show, duration represents the length of the show, listed_in tells what type and where the show belongs from, description gives short descriptions about the show. My task is to read and understand the data after that I will have to show some meaningfull charts and explain everything about the chart then according to the visualization chart I will have to make some hypothesis assumptions about the project then testing the assumptions. Then I will handle missing and null values and outliers after completing these all task I will look for imbalanced data if there is any imbalanced data then i will have to deal with that. Then i will select some important features further i will split the data for test and training purpose.

# **GitHub Link -** https://github.com/arunchaudhary147

# **Problem Statement**

This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.

In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.

Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.

## <b>In this  project, you are required to do </b>
1. Exploratory Data Analysis

2. Understanding what type content is available in different countries

3. Is Netflix has increasingly focusing on TV rather than movies in recent years.
4. Clustering similar content by matching text-based features

# ***Let's Begin !***

## ***1. Know Your Data***

### **Import Libraries**
"""

# Importing the libraries
import pandas as pd
import numpy as np
from numpy import math
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
from matplotlib.pyplot import figure
import plotly.graph_objects as go
import plotly.offline as py
import plotly.express as px
from datetime import datetime

import warnings
warnings.filterwarnings('ignore')

# Mounting the google drive to access the files
from google.colab import drive
drive.mount('/content/drive')

"""## **Dataset Loading**"""

# Loading the dataset
df = pd.read_csv('/content/drive/MyDrive/Dataset/NETFLIX MOVIES AND TV SHOWS CLUSTERING/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')

"""### **Dataset First View**"""

# Top 5 Rows
df.head()

# Last 5 Rows
df.tail()

"""
### **Dataset Rows & Columns count**"""

# Get the count of rows and columns
num_rows, num_columns = df.shape

# Display the count
print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_columns}")

"""We have 7787 Rows and 12 Column to Analyze.

### **Dataset Information**
"""

# Dataset Info
df.info()

# Checking the shape of the dataframe
df.shape

"""### **Duplicate Values**"""

# Dataset Duplicate Value Count
df[df.duplicated()]

"""### There are no duplicated values.

### **Missing Values/Null Values**
"""

# Missing Values/Null Values Count
df.isnull().sum().sum()

"""There are total 3631 null values in the dataset. 2389 null values in director column, 718 null values in cast column, 507 null values in country column, 10 in date_added column and 7 in rating column. So we need to handle the null values.

### **Handling Null Values** -
"""

# Checking null values
df.isnull().sum()

# Visualizing the missing values
# Create a DataFrame indicating missing values
missing_data = df.isnull()

# Set up the Matplotlib figure
plt.figure(figsize=(15, 10))

# Use seaborn to create a heatmap
sns.heatmap(missing_data, cmap='viridis', cbar=False)

# Add title and display the plot
plt.title('Visualizing Missing Data - Heatmap')
plt.show()

# Handling Null Values
# Fill missing values in 'cast' column with 'No cast'
df['cast'].fillna(value='No cast', inplace=True)

# Fill missing values in 'country' column with the mode (most frequent value) of the 'country' column
df['country'].fillna(value=df['country'].mode()[0], inplace=True)

# Drop rows where 'date_added' or 'rating' column has missing values
df.dropna(subset=['date_added', 'rating'], inplace=True)

#Dropping Director Column
df.drop(['director'],axis=1,inplace=True)

# Again checking is there any null values or not
df.isnull().sum()

#Again Checking Rows and Column count
# Get the count of rows and columns
num_rows, num_columns = df.shape

# Display the count
print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_columns}")

"""After dropping 'Director' Column and remove Rows with missing values in 'date_added' and 'rating' Columns we have 7770 Rows and 11 Columns.

### What did you know about your dataset?

*   The dataset contains information about TV shows and movies available on Netflix as of 2019. It was collected from Flixable, a third-party Netflix search engine. Notably, in 2018, a report indicated a significant increase in the number of TV shows on Netflix since 2010, while the number of movies had decreased.


*   Initially Dataset having 7787 rows and 12 columns. So, I decided to drop 'Director' column from the dataset to streamline and simplify the analysis, as this information is deemed less relevant to our specific goals.


*   After dropping 1 column and remove rows with missing values in 'date_added' and 'rating' Columns we have 7770 Rows and 11 Columns.

## ***2. Understanding Your Variables***
"""

# Columns name
df.columns

# Dataset Describe
dataset_description = df.describe(include='all').T

# Display the dataset description
print("\nSummary statistics for the entire dataset:")
print(dataset_description)

df["country"].value_counts().head(10)

"""##  **Variables Description**

1. show_id: Unique identifier assigned to every Movie/TV Show in the dataset.

2. type: Identifier indicating whether the entry is a Movie or TV Show.

3. title: The title of the Movie or TV Show.

4. director: The director of the Movie.

5. cast: The actors involved in the Movie or TV Show.

6. country: The country where the Movie or TV Show was produced.

7. date_added: The date when the Movie or TV Show was added on Netflix.

8. release_year: The actual release year of the Movie or TV Show.

9. rating: TV Rating assigned to the Movie or TV Show.

10. duration: The total duration, either in minutes for Movies or in the number of seasons for TV Shows.

11. listed_in: The genre or category classification of the Movie or TV Show.

12. description: A summary description providing additional information about the Movie or TV Show.
"""

# Display unique values for each variable
for column in df.columns:
    unique_values = df[column].unique()
    print(f"Unique values for '{column}':\n{unique_values}\n")

"""#**EDA**"""

df['type'].value_counts()

plt.figure(figsize=(10, 8))
sns.countplot(x='type', data=df, palette='Set2')  # You can choose a different palette

# Adding labels and title
plt.xlabel('Type')
plt.ylabel('Count')
plt.title('Count of Observations by Type')

# Rotating x-axis labels for better readability
plt.xticks(rotation=45)

# Display the plot
plt.show()

"""**Netflix has 5372 movies and 2398 TV shows,
there are more   number movies on Netflix than TV shows.**

"""

df['rating']

#Assigning the Ratings into grouped categories
ratings = {
    'TV-PG': 'Older Kids',
    'TV-MA': 'Adults',
    'TV-Y7-FV': 'Older Kids',
    'TV-Y7': 'Older Kids',
    'TV-14': 'Teens',
    'R': 'Adults',
    'TV-Y': 'Kids',
    'NR': 'Adults',
    'PG-13': 'Teens',
    'TV-G': 'Kids',
    'PG': 'Older Kids',
    'G': 'Kids',
    'UR': 'Adults',
    'NC-17': 'Adults'
}
df['target_ages'] = df['rating'].replace(ratings)

# type should be a category
df['type'] = pd.Categorical(df['type'])

# target_ages is another category (4 classes)
df['target_ages'] = pd.Categorical(df['target_ages'], categories=['Kids', 'Older Kids', 'Teens', 'Adults'])

df

#creating two extra columns
tv_shows=df[df['type']=='TV Show']
movies=df[df['type']=='Movie']

#Movie Ratings based on Target Age Groups
plt.figure(figsize=(14,6))
plt.title('movie ratings')
sns.countplot(x=movies['rating'],hue=movies['target_ages'],data=movies,order=movies['rating'].value_counts().index)

#Rating based on rating system of all TV Shows
tv_ratings = tv_shows.groupby(['rating'])['show_id'].count().reset_index(name='count').sort_values(by='count',ascending=False)
fig_dims = (14,7)
fig, ax = plt.subplots(figsize=fig_dims)
sns.pointplot(x='rating',y='count',data=tv_ratings)
plt.title('TV Show Ratings',size='20')
plt.show()

movies_year =movies['release_year'].value_counts().sort_index(ascending=False)

movies_year

tvshows_year =tv_shows['release_year'].value_counts().sort_index(ascending=False)

sns.set(font_scale=1.4)
movies_year.plot(figsize=(12, 8), linewidth=2.5, color='maroon',label="Movies / year",ms=3)
tvshows_year.plot(figsize=(12, 8), linewidth=2.5, color='blue',label="TV Shows / year")
plt.xlabel("Years", labelpad=15)
plt.ylabel("Number", labelpad=15)
plt.title("Production growth yearly", y=1.02, fontsize=22);

#Analysing how many movies released per year in last 15 years
plt.figure(figsize=(15,5))
sns.countplot(y=movies['release_year'],data=df,order=movies['release_year'].value_counts().index[0:20])

tvshows_year

#Analysing how many movies released per year in last 15 years
plt.figure(figsize=(15,5))
sns.countplot(y=tv_shows['release_year'],data=df,order=tv_shows['release_year'].value_counts().index[0:20])

"""The number of movies on Netflix is growing significantly faster than the number of TV shows.

In both 2018 and 2019, approximately 1200 new movies were added.

We saw a huge increase in the number of movies and television episodes after 2014.

Because of covid-19, there is a significant drop in the number of movies and television episodes produced after 2019.

It appears that Netflix has focused more attention on increasing Movie content that TV Shows. Movies have increased much more dramatically than TV shows.

"""

df

#adding columns of month and year of addition

df['month'] = pd.DatetimeIndex(df['date_added']).month
df.head()

# Plotting the Countplot
plt.figure(figsize=(12, 10))
ax = sns.countplot(x='month', data=df, palette='viridis')

# Adding labels and title
plt.xlabel('Month')
plt.ylabel('Count')
plt.title('Count of Observations by Month')

# Display the plot
plt.show()

fig, ax = plt.subplots(figsize=(15, 6))

sns.countplot(x='month', hue='type', lw=5, data=df, ax=ax, palette='Set2')

ax.set(xlabel='Month', ylabel='Count', title='Count of Observations by Month and Type')
ax.legend(title='Type', title_fontsize='14')

plt.show()

"""The above graph shows that the most content is added to Netflix in December.

"""

#Analysing top10 genre of the movies
plt.figure(figsize=(14, 6))
plt.title('Top 10 Genres of Movies', fontweight="bold")

sns.countplot(y='listed_in', data=movies, order=movies['listed_in'].value_counts().index[:10], palette="cubehelix")

plt.show()

# Analysing top10 genres of TVSHOWS
plt.figure(figsize=(14, 6))
plt.title('Top 10 Genres of TV Shows', fontweight="bold")

sns.countplot(y='listed_in', data=tv_shows, order=tv_shows['listed_in'].value_counts().index[:10], palette="viridis")

plt.show()

# Checking the distribution of Movie Durations
plt.figure(figsize=(10, 7))
sns.distplot(movies['duration'].str.extract('(\d+)'), kde=False, color='red')
plt.title('Distplot with Normal Distribution for Movies', fontweight="bold")
plt.show()

# Checking the distribution of TV SHOWS
plt.figure(figsize=(30, 6))
plt.title("Distribution of TV Shows Duration", fontweight='bold')

sns.countplot(x='duration', data=tv_shows, order=tv_shows['duration'].value_counts().index)

plt.show()

# Analysing top 15 countries with most content
plt.figure(figsize=(18, 5))
sns.countplot(x='country', data=df, palette="magma", order=df['country'].value_counts().index[:15], hue='type')
plt.xticks(rotation=50)
plt.title('Top 15 Countries with Most Content', fontsize=15, fontweight='bold')
plt.show()

# Top 2 countries where netflix is most popular
country=df['country'].value_counts().reset_index()
country

# Plotting the Horizontal bar plot for top 10 country contains Movie & TV Show split
country_order = df['country'].value_counts()[:11].index
content_data = df[['type', 'country']].groupby('country')['type'].value_counts().unstack().loc[country_order]
content_data['sum'] = content_data.sum(axis=1)
content_data_ratio = (content_data.T / content_data['sum']).T[['Movie', 'TV Show']].sort_values(by='Movie',ascending=False)[::-1]

# Plotting the bar
fig, ax = plt.subplots(1,1,figsize=(15, 8),)

ax.barh(content_data_ratio.index, content_data_ratio['Movie'],
        color='crimson', alpha=0.8, label='Movie')
ax.barh(content_data_ratio.index, content_data_ratio['TV Show'], left=content_data_ratio['Movie'],
        color='black', alpha=0.8, label='TV Show')

# Preparing data for heatmap
df['count'] = 1
data = df.groupby('country')[['country','count']].sum().sort_values(by='count',ascending=False).reset_index()[:10]
data = data['country']


df_heatmap = df.loc[df['country'].isin(data)]
df_heatmap = pd.crosstab(df_heatmap['country'],df_heatmap['target_ages'],normalize = "index").T
df_heatmap

# Plotting the heatmap
fig, ax = plt.subplots(1, 1, figsize=(12, 12))

country_order2 = ['United States', 'India', 'United Kingdom', 'Canada', 'Japan', 'France', 'South Korea', 'Spain',
       'Mexico']

age_order = ['Adults', 'Teens', 'Older Kids', 'Kids']

sns.heatmap(df_heatmap.loc[age_order,country_order2],cmap="YlGnBu",square=True, linewidth=2.5,cbar=False,
            annot=True,fmt='1.0%',vmax=.6,vmin=0.05,ax=ax,annot_kws={"fontsize":12})
plt.show()

"""The US and UK are closely aligned with their Netflix target ages, but radically different from eg. India or Japan!

Also, Mexico and Spain have similar content on Netflix for different age groups.
"""

# Extract numeric values from 'duration' and convert to numeric
movies['minute'] = movies['duration'].str.extract('(\d+)').astype(float)

# Calculate the average duration for each rating
duration_by_rating = movies.groupby(['rating'])['minute'].mean()

# Create a DataFrame and sort by average duration
duration_df = pd.DataFrame(duration_by_rating).sort_values('minute')

# Plotting the bar plot
plt.figure(figsize=(12, 6))
sns.barplot(x=duration_df.index, y=duration_df['minute'])

# Adding labels and title
plt.xlabel('Rating')
plt.ylabel('Average Duration (minutes)')
plt.title('Average Duration of Movies by Rating')

plt.show()

"""Those movies that have a rating of NC-17 have the longest average duration.

When it comes to movies having a TV-Y rating, they have the shortest runtime on average.
"""

df

df['date_added'] = pd.to_datetime(df['date_added'])
df['year_added'] = df['date_added'].dt.year
df

# All the movies and TV shows in different dataframe
movies = df[df['type']=='Movie']
tv_shows= df[df['type']=='TV Show']

# Adding a column to identify original movies
movies['originals'] = np.where(movies['release_year'] == movies['year_added'], 'Yes', 'No')

# Pie plot showing the percentage of originals and others in movies
fig, ax = plt.subplots(figsize=(5, 5), facecolor="#363336")
ax.patch.set_facecolor('#363336')

# Explode the 'Originals' slice for emphasis
explode = (0, 0.1)

# Define colors for the slices
colors = ['red', '#F5E9F5']

# Create the pie plot
ax.pie(movies['originals'].value_counts(), explode=explode, autopct='%.2f%%', labels=['Others', 'Originals'],
       shadow=True, startangle=90, textprops={'color': "black", 'fontsize': 20}, colors=colors)

plt.show()

"""30% movies released on Netflix.
70% movies added on Netflix were released earlier by different mode.

#**Making some hypothesis**

HO: Netflix has more movies than TV Shows.
H1:Netflix not have more movies than TV SHOWS.
"""

df['type'].value_counts()

movies

#creating two extra columns
tv_shows=df[df['type']=='TV Show']
movies=df[df['type']=='Movie']

np.mean(movies)

movies.duration

#making copy of df_clean_frame
df_hypothesis=df.copy()
#head of df_hypothesis
df_hypothesis.head()

#filtering movie from Type_of_show column
df_hypothesis = df_hypothesis[df_hypothesis["type"] == "Movie"]

#with respect to each ratings assigning it into group of categories
ratings_ages = {
    'TV-PG': 'Older Kids',
    'TV-MA': 'Adults',
    'TV-Y7-FV': 'Older Kids',
    'TV-Y7': 'Older Kids',
    'TV-14': 'Teens',
    'R': 'Adults',
    'TV-Y': 'Kids',
    'NR': 'Adults',
    'PG-13': 'Teens',
    'TV-G': 'Kids',
    'PG': 'Older Kids',
    'G': 'Kids',
    'UR': 'Adults',
    'NC-17': 'Adults'
}

df_hypothesis['target_ages'] = df_hypothesis['rating'].replace(ratings_ages)
#let's see unique target ages
df_hypothesis['target_ages'].unique()

#Another category is target_ages (4 classes).
df_hypothesis['target_ages'] = pd.Categorical(df_hypothesis['target_ages'], categories=['Kids', 'Older Kids', 'Teens', 'Adults'])
#from duration feature extractin string part and after extracting Changing the object type to numeric
df_hypothesis['duration']= df_hypothesis['duration'].str.extract('(\d+)')
df_hypothesis['duration'] = pd.to_numeric(df_hypothesis['duration'])
#head of df_
df_hypothesis.head(3)

# Group_by duration and target_ages
group_by_= df_hypothesis[['duration','target_ages']].groupby(by='target_ages')

#mean of group_by variable
group=group_by_.mean().reset_index()
group

#In A and B variable grouping values
A= group_by_.get_group('Kids')
B= group_by_.get_group('Older Kids')
#mean and std. calutation for kids and older kids variables
M1 = A.mean()
S1 = A.std()

M2= B.mean()
S2 = B.std()

print('Mean for movies rated for Kids {} \n Mean for  movies rated for older kids {}'.format(M1,M2))
print('Std for  movies rated for Older Kids {} \n Std for  movies rated for kids {}'.format(S2,S1))

from scipy.stats import ttest_ind
ttest,pval = ttest_ind(M1, M2)
print(pval)
if pval<0.05:
    print("reject null hypothesis")
else:
    print("accept null hypothesis")

#import stats
from scipy import stats
#length of groups and DOF
n1 = len(A)
n2= len(B)
print(n1,n2)

dof = n1+n2-2
print('dof',dof)

sp_2 = ((n2-1)*S1**2  + (n1-1)*S2**2) / dof
print('SP_2 =',sp_2)

sp = np.sqrt(sp_2)
print('SP',sp)

#tvalue
t_val = (M1-M2)/(sp * np.sqrt(1/n1 + 1/n2))
print('tvalue',t_val[0])

# t-distribution
stats.t.ppf(0.025,dof)

#t-distribution
stats.t.ppf(0.975,dof)

"""Because the t-value is not in the range, the null hypothesis is rejected.

As a result, movies rated for kids and older kids are not at least two hours long.
"""

# Making copy of df_clean_frame
df_hypothesis = df.copy()

# Displaying the head of df_hypothesis
df_hypothesis.head()

# Extract numeric values from 'duration' using regular expressions
df_hypothesis['duration'] = df_hypothesis['duration'].str.extract('(\d+)')

# Convert the extracted values to numeric format with error handling
df_hypothesis['duration'] = pd.to_numeric(df_hypothesis['duration'], errors='coerce')

#Another category is target_ages (4 classes).
df_hypothesis['type'] = pd.Categorical(df_hypothesis['type'], categories=['Movie','TV Show'])
#from duration feature extractin string part and after extracting Changing the object type to numeric
#df_hypothesis['duration']= df_hypothesis['duration'].str.extract('(\d+)')
#df_hypothesis['duration'] = pd.to_numeric(df_hypothesis['duration'])
#head of df_
df_hypothesis.head(3)

#group_by duration and target_ages
group_by_= df_hypothesis[['duration','type']].groupby(by='type')
#mean of group_by variable
group=group_by_.mean().reset_index()
group

#In A and B variable grouping values
A= group_by_.get_group('Movie')
B= group_by_.get_group('TV Show')
#mean and std. calutation for kids and older kids variables
M1 = A.mean()
S1 = A.std()

M2= B.mean()
S2 = B.std()

print('Mean for movies rated for Kids {} \n Mean for  movies rated for older kids {}'.format(M1,M2))
print('Std for  movies rated for Older Kids {} \n Std for  movies rated for kids {}'.format(S2,S1))

#import stats
from scipy import stats
#length of groups and DOF
n1 = len(A)
n2= len(B)
print(n1,n2)

dof = n1+n2-2
print('dof',dof)

sp_2 = ((n2-1)*S1**2  + (n1-1)*S2**2) / dof
print('SP_2 =',sp_2)

sp = np.sqrt(sp_2)
print('SP',sp)

#tvalue
t_val = (M1-M2)/(sp * np.sqrt(1/n1 + 1/n2))
print('tvalue',t_val[0])

#t-distribution
stats.t.ppf(0.025,dof)

#t-distribution
stats.t.ppf(0.975,dof)

"""Because the t-value is not in the range, the null hypothesis is rejected.

As a result, The duration which is more than 90 mins are movies.
"""

df.dtypes

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.snowball import SnowballStemmer
nltk.download('punkt')

df.dtypes

df['description'].astype(str)

# After above all the changes, those features are in list format, so making list of description feature
df['description'] = df['description'].apply(lambda x: str(x).split(' '))

# Converting text feature to string from list
df['description']= df['description'].apply(lambda x: " ".join(x))

# making all the words in text feature to lowercase
df['description']= df['description'].apply(lambda x: x.lower())

def remove_punctuation(text):
    '''a function for removing punctuation'''
    import string
    # replacing the punctuations with no space,
    # which in effect deletes the punctuation marks
    translator = str.maketrans('', '', string.punctuation)
    # return the text stripped of punctuation marks
    return text.translate(translator)
# applying above function on text feature
df['description']= df['description'].apply(remove_punctuation)

df['description'][0:10]

# using nltk library to download stopwords
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
sw=stopwords.words('english')
#Defining stopwords
def stopwords(text):
    '''a function for removing the stopword'''
    text = [word for word in text.split() if word not in sw]
    # joining the list of words with space separator
    return " ".join(text)
# applying above function on text feature
df['description']=df['description'].apply(stopwords)
# this is how value in text looks like after removing stopwords
df['description'][0]

# importing TfidVectorizer from sklearn library
from sklearn.feature_extraction.text import TfidfVectorizer

# Applying Tfidf Vectorizer
tfidf_model = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf_model.fit_transform(df['description'])

# Converting the TF-IDF matrix to a dense array if needed
X = X_tfidf.toarray()

# Displaying the shape of the transformed matrix
print("Shape of X:", X.shape)

#finding optimal number of clusters using the elbow method
from sklearn.cluster import KMeans
wcss_list= []  #Initializing the list for the values of WCSS

#Using for loop for iterations from 1 to 30.
for i in range(1, 30):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)
    kmeans.fit(X)
    wcss_list.append(kmeans.inertia_)
plt.plot(range(1, 30), wcss_list)
plt.title('The Elbow Method Graph')
plt.xlabel('Number of clusters(k)')
plt.ylabel('wcss_list')
plt.show()

from sklearn.metrics import silhouette_score
#sillhoute score of clusters
sill = []
for i in range(2,30):
    model = KMeans(n_clusters=i,init ='k-means++',random_state=51)
    model.fit(X)
    y1 = model.predict(X)
    score = silhouette_score(X,y1)
    sill.append(score)
    print('cluster: %d \t Sillhoute: %0.4f'%(i,score))

#Plotting Sillhoute's score
plt.plot(sill,'bs--')
plt.xticks(list(range(2,30)))
plt.grid(),plt.xlabel('Number of cluster')
plt.show()

#training the K-means model on a dataset
kmeans = KMeans(n_clusters= 26, init='k-means++', random_state= 42)
y_predict= kmeans.fit_predict(X)

# Predict the clusters and evaluate the silhouette score

score = silhouette_score(X, y_predict)
print("Silhouette score is {}".format(score))

#Adding a seperate column for the cluster
df["cluster"] = y_predict

df['cluster'].value_counts()

fig, ax = plt.subplots(figsize=(15, 6))
sns.countplot(x='cluster', hue='type', data=df, palette='tab10', ax=ax)
plt.title('Count Plot of Clusters by Type')
plt.show()

# Scatter plot for Clusters
fig = px.scatter(df, y="description", x="cluster",color="cluster")
fig.update_traces(marker_size=100)
fig.show()

import scipy.cluster.hierarchy as shc
plt.figure(figsize =(8, 8))
plt.title('Visualising the data')
Dendrogram = shc.dendrogram((shc.linkage(X, method ='ward')))

#Fitting our variable in Agglomerative Clusters
from sklearn.cluster import AgglomerativeClustering
aggh = AgglomerativeClustering(n_clusters=6, affinity='euclidean', linkage='ward')
aggh.fit(X)
#Predicting using our model
y_hc=aggh.fit_predict(X)

df_hierarchical =df.copy()
#creating a column where each row is assigned to their separate cluster
df_hierarchical['cluster'] = aggh.labels_
df_hierarchical.head()

#Analysing number of points present in each clusters
plt.bar([x for x in range(6)], df_hierarchical.groupby(['cluster'])['description'].count(), alpha = 0.4)
plt.title('KMeans cluster points')
plt.xlabel("Cluster number")
plt.ylabel("Number of points")
plt.show()

#Silhouette Coefficient
print("Silhouette Coefficient: %0.3f"%silhouette_score(X, aggh.labels_, metric='euclidean'))

"""# **Conclusion**

● From elbow and sillhoute score, optimal of 26 clusters formed , K Means is best for identification than Hierarchical as the evaluation metrics also indicates the same.In kmean cluster 0 has the highest number of datapoints and evenly distributed for other cluster.

● Netflix has 5372 movies and 2398 TV shows, there are more movies on Netflix than TV shows.

● TV-MA has the highest number of ratings for tv shows i,e adult ratings.

● Highest number of movies released in 2017 and 2018 highest number of movies released in 2020 The number of movies on Netflix is growing significantly faster than the number of TV shows. We saw a huge increase in the number of movies and television episodes after 2015. There is a significant drop in the number of movies and television episodes produced after 2020. It appears that Netflix has focused more attention on increasing Movie content than TV Shows. Movies have increased much more dramatically than TV shows.

● The most content is added to Netflix from october to january.

● Documentaries are the top most genre in netflix which is followed by standup comedy and Drama and international movies.

● Kids tv is the top most TV show genre in netflix.

● Most of the movies have duration of between 50 to 150.

● Highest number of tv_shows consisting of single season.

● Those movies that have a rating of NC-17 have the longest average duration.

● When it comes to movies having a TV-Y rating, they have the shortest runtime on average.

● United states has the highest number of content on the netflix ,followed by india.

● India has highest number of movies in netflix.

● 30% movies released on Netflix. 70% movies added on Netflix were released earlier by different mode.
"""